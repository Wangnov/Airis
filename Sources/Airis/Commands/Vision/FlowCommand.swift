import ArgumentParser
@preconcurrency import Vision
import CoreImage
import Foundation

struct FlowCommand: AsyncParsableCommand {
    static let configuration = CommandConfiguration(
        commandName: "flow",
        abstract: HelpTextFactory.text(
            en: "Analyze optical flow between two images",
            cn: "åˆ†æä¸¤å¼ å›¾ç‰‡ä¹‹é—´çš„å…‰æµ"
        ),
        discussion: helpDiscussion(
            en: """
                Calculate pixel motion vectors between two consecutive frames or images.
                This is useful for motion estimation, video analysis, and tracking.

                QUICK START:
                  airis vision flow frame1.jpg frame2.jpg

                HOW IT WORKS:
                  Optical flow computes the apparent motion of pixels between two images.
                  The result is a vector field where each pixel has X and Y displacement values.

                EXAMPLES:
                  # Basic optical flow analysis
                  airis vision flow prev.png next.png

                  # High accuracy analysis
                  airis vision flow frame1.jpg frame2.jpg --accuracy high

                  # Save flow visualization
                  airis vision flow prev.png next.png -o flow_viz.png

                  # JSON output for scripting
                  airis vision flow prev.png next.png --format json

                ACCURACY LEVELS:
                  low       - Fastest, lower precision
                  medium    - Balanced (default)
                  high      - Higher precision
                  veryHigh  - Best precision, slowest

                OUTPUT EXAMPLE:
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                  Optical Flow Analysis
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                  Input: frame1.jpg -> frame2.jpg
                  Flow field: 1920 x 1080
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

                NOTE:
                  Works best with consecutive video frames or images with moderate motion.
                  Large displacements may reduce accuracy.
                """,
            cn: """
                è®¡ç®—ä¸¤å¼ å›¾ç‰‡ï¼ˆå¸¸ç”¨äºè¿ç»­å¸§ï¼‰ä¹‹é—´çš„åƒç´ è¿åŠ¨å‘é‡ï¼ˆå…‰æµï¼‰ã€‚
                é€‚ç”¨äºè¿åŠ¨ä¼°è®¡ã€è§†é¢‘åˆ†æã€ç›®æ ‡è·Ÿè¸ªç­‰åœºæ™¯ã€‚

                QUICK START:
                  airis vision flow frame1.jpg frame2.jpg

                EXAMPLES:
                  # åŸºç¡€å…‰æµåˆ†æ
                  airis vision flow prev.png next.png

                  # æ›´é«˜ç²¾åº¦
                  airis vision flow frame1.jpg frame2.jpg --accuracy high

                  # è¾“å‡ºå¯è§†åŒ–å›¾ï¼ˆPNGï¼‰
                  airis vision flow prev.png next.png -o flow_viz.png

                  # JSON è¾“å‡ºï¼ˆä¾¿äºè„šæœ¬è§£æï¼‰
                  airis vision flow prev.png next.png --format json

                ACCURACY LEVELS:
                  low / mediumï¼ˆé»˜è®¤ï¼‰/ high / veryHigh
                """
        )
    )

    @Argument(help: HelpTextFactory.help(en: "First image (source/previous frame)", cn: "ç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆä¸Šä¸€å¸§/å‚è€ƒï¼‰"))
    var image1: String

    @Argument(help: HelpTextFactory.help(en: "Second image (target/next frame)", cn: "ç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆä¸‹ä¸€å¸§/ç›®æ ‡ï¼‰"))
    var image2: String

    @Option(name: [.short, .long], help: HelpTextFactory.help(en: "Output flow visualization (PNG)", cn: "è¾“å‡ºå…‰æµå¯è§†åŒ–å›¾è·¯å¾„ï¼ˆPNGï¼‰"))
    var output: String?

    @Option(
        name: .long,
        help: HelpTextFactory.help(
            en: "Computation accuracy (low, medium, high, veryHigh)",
            cn: "è®¡ç®—ç²¾åº¦ï¼ˆlow / medium / high / veryHighï¼‰"
        )
    )
    var accuracy: String = "medium"

    @Option(name: .long, help: HelpTextFactory.help(en: "Output format (table, json)", cn: "è¾“å‡ºæ ¼å¼ï¼ˆtable / jsonï¼‰"))
    var format: String = "table"

    func run() async throws {
        let url1 = try FileUtils.validateImageFile(at: image1)
        let url2 = try FileUtils.validateImageFile(at: image2)
        let outputFormat = OutputFormat.parse(format)
        let showHumanOutput = AirisOutput.shouldPrintHumanOutput(format: outputFormat)

        // Parse accuracy level
        let accuracyLevel: VisionService.OpticalFlowAccuracy
        switch accuracy.lowercased() {
        case "low":
            accuracyLevel = .low
        case "medium":
            accuracyLevel = .medium
        case "high":
            accuracyLevel = .high
        case "veryhigh":
            accuracyLevel = .veryHigh
        default:
            accuracyLevel = .medium
        }

        if showHumanOutput {
            print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            print("ğŸŒŠ Optical Flow Analysis")
            print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            print("ğŸ“ Input: \(url1.lastPathComponent) â†’ \(url2.lastPathComponent)")
            print("ğŸ¯ Accuracy: \(accuracy)")
            if let output = output {
                print("ğŸ’¾ Output: \(output)")
            }
            print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            print("")
            print("â³ Processing...")
        }

        let result: VisionService.OpticalFlowResult
        #if DEBUG
        if ProcessInfo.processInfo.environment["AIRIS_TEST_FLOW_FAKE_RESULT"] == "1" {
            result = Self.testFlowResult()
        } else {
            let vision = ServiceContainer.shared.visionService
            result = try await vision.computeOpticalFlow(
                from: url1,
                to: url2,
                accuracy: accuracyLevel
            )
        }
        #else
        let vision = ServiceContainer.shared.visionService
        result = try await vision.computeOpticalFlow(
            from: url1,
            to: url2,
            accuracy: accuracyLevel
        )
        #endif

        if outputFormat == .json {
            printJSON(result: result, file1: url1.lastPathComponent, file2: url2.lastPathComponent)
        } else if showHumanOutput {
            print("")
            print("âœ… Analysis complete")
            print("")
            print("Flow field: \(result.width) Ã— \(result.height)")
        }

        // Save visualization if requested
        if let outputPath = output {
            try saveFlowVisualization(result: result, to: outputPath)
            if showHumanOutput {
                print("")
                print(Strings.get("info.saved_to", outputPath))
            }
        }
    }

    private func printJSON(result: VisionService.OpticalFlowResult, file1: String, file2: String) {
        let dict: [String: Any] = [
            "source": file1,
            "target": file2,
            "flow_field": [
                "width": result.width,
                "height": result.height
            ],
            "accuracy": accuracy
        ]

        if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
           let jsonString = String(data: jsonData, encoding: .utf8) {
            print(jsonString)
        }
    }

    private func saveFlowVisualization(result: VisionService.OpticalFlowResult, to outputPath: String) throws {
        // Convert flow buffer to grayscale visualization
        let flowImage = CIImage(cvPixelBuffer: result.pixelBuffer)

        // Apply a simple visualization: convert to grayscale magnitude
        let colorSpace = CGColorSpaceCreateDeviceRGB()
        let context = CIContext()

        // Scale the flow field to match a visible range
        let scaledImage = flowImage.transformed(by: CGAffineTransform(scaleX: 1, y: 1))

        #if DEBUG
        let forceNil = ProcessInfo.processInfo.environment["AIRIS_FORCE_FLOW_CGIMAGE_NIL"] == "1"
        let cgImageCandidate: CGImage?
        if forceNil {
            cgImageCandidate = nil
        } else {
            cgImageCandidate = context.createCGImage(scaledImage, from: scaledImage.extent, format: .RGBAf, colorSpace: colorSpace)
        }
        #else
        let cgImageCandidate = context.createCGImage(scaledImage, from: scaledImage.extent, format: .RGBAf, colorSpace: colorSpace)
        #endif

        guard let cgImage = cgImageCandidate else {
            throw AirisError.imageEncodeFailed
        }

        let imageIO = ServiceContainer.shared.imageIOService
        let outputURL = URL(fileURLWithPath: outputPath)

        // Ensure directory exists
        try FileUtils.ensureDirectory(for: outputPath)

        try imageIO.saveImage(cgImage, to: outputURL, format: "png")
    }

    #if DEBUG
    /// æµ‹è¯•æ¡©ï¼šå¿«é€Ÿç”Ÿæˆ 2x2 å…‰æµç»“æœï¼Œé¿å…ä¾èµ– Vision å®é™…è®¡ç®—
    private static func testFlowResult() -> VisionService.OpticalFlowResult {
        let forceCreateFailure = ProcessInfo.processInfo.environment["AIRIS_FORCE_FLOW_TEST_PIXELBUFFER_FAIL"] == "1"

        var pixelBuffer: CVPixelBuffer?
        let status: CVReturn = forceCreateFailure
            ? kCVReturnInvalidSize
            : CVPixelBufferCreate(nil, 2, 2, kCVPixelFormatType_32BGRA, nil, &pixelBuffer)

        if status == kCVReturnSuccess, let buffer = pixelBuffer {
            return VisionService.OpticalFlowResult(pixelBuffer: buffer, width: 2, height: 2)
        }

        // ç†è®ºä¸Šä¸åº”è§¦å‘ï¼›è‹¥è§¦å‘åˆ™æŒç»­å°è¯•ç›´åˆ°åˆ›å»ºæˆåŠŸï¼ˆæµ‹è¯•æ¡©ä»…ç”¨äºè¦†ç›–ä¸é¿å… fatalErrorï¼‰ã€‚
        var retryPixelBuffer: CVPixelBuffer!
        while retryPixelBuffer == nil {
            var retryBuffer: CVPixelBuffer?
            let retryStatus = CVPixelBufferCreate(nil, 2, 2, kCVPixelFormatType_32BGRA, nil, &retryBuffer)
            if retryStatus == kCVReturnSuccess, let retryBuffer {
                retryPixelBuffer = retryBuffer
            }
        }

        return VisionService.OpticalFlowResult(pixelBuffer: retryPixelBuffer, width: 2, height: 2)
    }
    #endif
}
