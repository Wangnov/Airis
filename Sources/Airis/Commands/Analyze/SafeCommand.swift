import ArgumentParser
import SensitiveContentAnalysis
import Foundation

struct SafeCommand: AsyncParsableCommand {
    static let configuration = CommandConfiguration(
        commandName: "safe",
        abstract: "Detect sensitive content in images",
        discussion: """
            Analyze images for sensitive content (nudity) using Apple's
            SensitiveContentAnalysis framework.

            âš ï¸  IMPORTANT REQUIREMENTS:
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            This feature requires ALL of the following:

            1. macOS 14.0 or later

            2. System setting enabled:
               System Settings > Privacy & Security > Sensitive Content Warning

            3. App signed with PAID Apple Developer Program:
               - Free developer accounts CANNOT use this feature
               - Requires entitlement: com.apple.developer.sensitivecontentanalysis.client
               - CLI must be code-signed with Developer ID

            If ANY requirement is not met, the command will show a warning
            and exit without analysis.
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            QUICK START:
              airis analyze safe photo.jpg

            EXAMPLES:
              # Basic sensitive content check
              airis analyze safe photo.jpg

              # JSON output for scripting
              airis analyze safe image.png --format json

              # Batch checking (use shell loop)
              for f in *.jpg; do airis analyze safe "$f" --format json; done

            OUTPUT FORMAT (table):
              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹
              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              ğŸ“ æ–‡ä»¶: photo.jpg
              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

              âœ… æœªæ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹

            OUTPUT FORMAT (json):
              {
                "file": "photo.jpg",
                "is_sensitive": false
              }

            PRIVACY NOTES:
              - All analysis is performed locally on device
              - Results are never transmitted off-device
              - This feature respects user privacy settings
            """
    )

    @Argument(help: "Path to the image file")
    var imagePath: String

    @Option(name: .long, help: "Output format: table (default), json")
    var format: String = "table"

    func run() async throws {
        let testMode = ProcessInfo.processInfo.environment["AIRIS_TEST_MODE"] == "1"
        let forcePolicyDisabled = ProcessInfo.processInfo.environment["AIRIS_SAFE_POLICY_DISABLED"] == "1"
        let forceSensitive = ProcessInfo.processInfo.environment["AIRIS_SAFE_FORCE_SENSITIVE"] == "1"
        let filename = URL(fileURLWithPath: imagePath).lastPathComponent
        let url = try FileUtils.validateImageFile(at: imagePath)

        // æ˜¾ç¤ºå‚æ•°æ€»è§ˆï¼ˆæµ‹è¯•æ¨¡å¼ä¹Ÿæ‰“å°ï¼Œä¿æŒä¸€è‡´ï¼‰
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(testMode ? "ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹ (TEST MODE)" : "ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("ğŸ“ æ–‡ä»¶: \(url.lastPathComponent)")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("")

        // é€‰æ‹©åˆ†æå™¨æˆ–æµ‹è¯•æ¡©
        let policy: SCSensitivityAnalysisPolicy
        let isSensitive: Bool

        if testMode {
            policy = forcePolicyDisabled ? .disabled : .simpleInterventions
            isSensitive = forceSensitive
        } else {
            #if DEBUG
            // æµ‹è¯•/è°ƒè¯•æ„å»ºèµ°è½»é‡æ¡©ï¼Œé¿å…ä¾èµ–çœŸå®æ•æ„Ÿå†…å®¹åˆ†æï¼ˆéœ€ç­¾å & ç³»ç»Ÿè®¾ç½®ï¼‰
            policy = .simpleInterventions
            isSensitive = false
            #else
            let analyzer = SCSensitivityAnalyzer()
            policy = analyzer.analysisPolicy
            if policy == .disabled {
                print(Strings.get("safe.disabled_hint"))
                return
            }
            let result = try await analyzer.analyzeImage(at: url)
            isSensitive = result.isSensitive
            #endif
        }

        if policy == .disabled {
            // æµ‹è¯•æ¨¡å¼ä¸‹å¼ºåˆ¶è¦†ç›– policy åˆ†æ”¯
            print(Strings.get("safe.disabled_hint"))
            return
        }

        outputResult(isSensitive: isSensitive, filename: filename)
    }

    private func outputResult(isSensitive: Bool, filename: String) {
        if format.lowercased() == "json" {
            let dict: [String: Any] = [
                "file": filename,
                "is_sensitive": isSensitive
            ]

            if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
               let jsonString = String(data: jsonData, encoding: .utf8) {
                print(jsonString)
            }
            return
        }

        // table
        if isSensitive {
            print("âš ï¸  æ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹")
        } else {
            print("âœ… " + Strings.get("safe.is_safe"))
        }
    }
}
