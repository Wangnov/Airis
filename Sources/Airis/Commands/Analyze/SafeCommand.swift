import ArgumentParser
import SensitiveContentAnalysis
import Foundation

struct SafeCommand: AsyncParsableCommand {
    static var configuration: CommandConfiguration {
        CommandConfiguration(
        commandName: "safe",
        abstract: HelpTextFactory.text(
            en: "Detect sensitive content in images",
            cn: "æ£€æµ‹å›¾ç‰‡æ˜¯å¦åŒ…å«æ•æ„Ÿå†…å®¹"
        ),
        discussion: helpDiscussion(
            en: """
                Analyze images for sensitive content (nudity) using Apple's
                SensitiveContentAnalysis framework.

                âš ï¸  IMPORTANT REQUIREMENTS:
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                This feature requires ALL of the following:

                1. macOS 14.0 or later

                2. System setting enabled:
                   System Settings > Privacy & Security > Sensitive Content Warning

                3. App signed with PAID Apple Developer Program:
                   - Free developer accounts CANNOT use this feature
                   - Requires entitlement: com.apple.developer.sensitivecontentanalysis.client
                   - CLI must be code-signed with Developer ID

                If ANY requirement is not met, the command will show a warning
                and exit without analysis.
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                QUICK START:
                  airis analyze safe photo.jpg

                EXAMPLES:
                  # Basic sensitive content check
                  airis analyze safe photo.jpg

                  # JSON output for scripting
                  airis analyze safe image.png --format json

                  # Batch checking (use shell loop)
                  for f in *.jpg; do airis analyze safe "$f" --format json; done

                OUTPUT FORMAT (table):
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                  ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                  ğŸ“ æ–‡ä»¶: photo.jpg
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

                  âœ… æœªæ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹

                OUTPUT FORMAT (json):
                  {
                    "file": "photo.jpg",
                    "is_sensitive": false
                  }

                PRIVACY NOTES:
                  - All analysis is performed locally on device
                  - Results are never transmitted off-device
                  - This feature respects user privacy settings
                """,
            cn: """
                ä½¿ç”¨ SensitiveContentAnalysis æ£€æµ‹å›¾ç‰‡æ˜¯å¦åŒ…å«æ•æ„Ÿå†…å®¹ï¼ˆå¦‚è£¸éœ²ï¼‰ã€‚

                âš ï¸  é‡è¦è¦æ±‚ï¼ˆç¼ºä¸€ä¸å¯ï¼‰ï¼š
                  1) macOS 14.0+
                  2) ç³»ç»Ÿè®¾ç½®å·²å¼€å¯ï¼š
                     ç³»ç»Ÿè®¾ç½® > éšç§ä¸å®‰å…¨æ€§ > æ•æ„Ÿå†…å®¹è­¦å‘Š
                  3) éœ€è¦ä»˜è´¹ Apple Developer Program ç­¾åï¼š
                     - å…è´¹å¼€å‘è€…è´¦å·æ— æ³•ä½¿ç”¨
                     - éœ€è¦ entitlement: com.apple.developer.sensitivecontentanalysis.client
                     - CLI éœ€ä½¿ç”¨ Developer ID è¿›è¡Œä»£ç ç­¾å

                QUICK START:
                  airis analyze safe photo.jpg

                EXAMPLES:
                  # åŸºç¡€æ£€æµ‹
                  airis analyze safe photo.jpg

                  # JSON è¾“å‡ºï¼ˆä¾¿äºè„šæœ¬è§£æï¼‰
                  airis analyze safe image.png --format json

                  # æ‰¹é‡æ£€æµ‹ï¼ˆshell ç¤ºä¾‹ï¼‰
                  for f in *.jpg; do airis analyze safe "$f" --format json; done

                éšç§è¯´æ˜ï¼š
                  - å…¨éƒ¨æœ¬åœ°æ‰§è¡Œï¼ˆä¸ä¸Šä¼ å›¾ç‰‡ï¼‰
                  - ç»“æœä¸ä¼šç¦»å¼€è®¾å¤‡
                """
        )
    )
    }

    @Argument(help: HelpTextFactory.help(en: "Path to the image file", cn: "è¾“å…¥å›¾ç‰‡è·¯å¾„"))
    var imagePath: String

    @Option(name: .long, help: HelpTextFactory.help(en: "Output format: table (default), json", cn: "è¾“å‡ºæ ¼å¼ï¼štableï¼ˆé»˜è®¤ï¼‰æˆ– json"))
    var format: String = "table"

    func run() async throws {
        let testMode = ProcessInfo.processInfo.environment["AIRIS_TEST_MODE"] == "1"
        let forcePolicyDisabled = ProcessInfo.processInfo.environment["AIRIS_SAFE_POLICY_DISABLED"] == "1"
        let forceSensitive = ProcessInfo.processInfo.environment["AIRIS_SAFE_FORCE_SENSITIVE"] == "1"
        let filename = URL(fileURLWithPath: imagePath).lastPathComponent
        let url = try FileUtils.validateImageFile(at: imagePath)

        let outputFormat = OutputFormat.parse(format)
        let showHumanOutput = AirisOutput.shouldPrintHumanOutput(format: outputFormat)

        AirisOutput.printBanner([
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            testMode ? "ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹ (TEST MODE)" : "ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "ğŸ“ æ–‡ä»¶: \(url.lastPathComponent)",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        ], enabled: showHumanOutput)

        // é€‰æ‹©åˆ†æå™¨æˆ–æµ‹è¯•æ¡©
        let policy: SCSensitivityAnalysisPolicy
        let isSensitive: Bool

        if testMode {
            policy = forcePolicyDisabled ? .disabled : .simpleInterventions
            isSensitive = forceSensitive
        } else {
            #if DEBUG
            // æµ‹è¯•/è°ƒè¯•æ„å»ºèµ°è½»é‡æ¡©ï¼Œé¿å…ä¾èµ–çœŸå®æ•æ„Ÿå†…å®¹åˆ†æï¼ˆéœ€ç­¾å & ç³»ç»Ÿè®¾ç½®ï¼‰
            policy = .simpleInterventions
            isSensitive = false
            #else
            let analyzer = SCSensitivityAnalyzer()
            policy = analyzer.analysisPolicy
            if policy == .disabled {
                if outputFormat == .json {
                    printDisabledJSON(filename: filename)
                } else if showHumanOutput {
                    print(Strings.get("safe.disabled_hint"))
                }
                return
            }
            let result = try await analyzer.analyzeImage(at: url)
            isSensitive = result.isSensitive
            #endif
        }

        if policy == .disabled {
            // æµ‹è¯•æ¨¡å¼ä¸‹å¼ºåˆ¶è¦†ç›– policy åˆ†æ”¯
            if outputFormat == .json {
                printDisabledJSON(filename: filename)
            } else if showHumanOutput {
                print(Strings.get("safe.disabled_hint"))
            }
            return
        }

        outputResult(isSensitive: isSensitive, filename: filename, outputFormat: outputFormat, showHumanOutput: showHumanOutput)
    }

    private func outputResult(isSensitive: Bool, filename: String, outputFormat: OutputFormat, showHumanOutput: Bool) {
        if outputFormat == .json {
            let dict: [String: Any] = [
                "file": filename,
                "is_sensitive": isSensitive
            ]

            if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
               let jsonString = String(data: jsonData, encoding: .utf8) {
                print(jsonString)
            }
            return
        }

        // table
        guard showHumanOutput else { return }

        if isSensitive {
            print("âš ï¸  æ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹")
        } else {
            print("âœ… " + Strings.get("safe.is_safe"))
        }
    }

    private func printDisabledJSON(filename: String) {
        let dict: [String: Any] = [
            "file": filename,
            "supported": false,
            "error": "policy_disabled"
        ]
        if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
           let jsonString = String(data: jsonData, encoding: .utf8) {
            print(jsonString)
        }
    }
}
