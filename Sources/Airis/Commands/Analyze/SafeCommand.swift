import ArgumentParser
import SensitiveContentAnalysis
import Foundation

struct SafeCommand: AsyncParsableCommand {
    static let configuration = CommandConfiguration(
        commandName: "safe",
        abstract: "Detect sensitive content in images",
        discussion: """
            Analyze images for sensitive content (nudity) using Apple's
            SensitiveContentAnalysis framework.

            QUICK START:
              airis analyze safe photo.jpg

            EXAMPLES:
              # Basic sensitive content check
              airis analyze safe photo.jpg

              # JSON output for scripting
              airis analyze safe image.png --format json

              # Batch checking (use shell loop)
              for f in *.jpg; do airis analyze safe "$f" --format json; done

            OUTPUT FORMAT (table):
              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹
              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              ğŸ“ æ–‡ä»¶: photo.jpg
              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

              âœ… æœªæ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹

            OUTPUT FORMAT (json):
              {
                "file": "photo.jpg",
                "is_sensitive": false
              }

            REQUIREMENTS:
              macOS 14.0 or later
              User must enable: System Settings > Privacy & Security
                               > Sensitive Content Warning

            PRIVACY NOTES:
              - All analysis is performed locally on device
              - Results are never transmitted off-device
              - This feature respects user privacy settings
            """
    )

    @Argument(help: "Path to the image file")
    var imagePath: String

    @Option(name: .long, help: "Output format: table (default), json")
    var format: String = "table"

    func run() async throws {
        let url = try FileUtils.validateImageFile(at: imagePath)

        // æ˜¾ç¤ºå‚æ•°æ€»è§ˆ
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("ğŸ“ æ–‡ä»¶: \(url.lastPathComponent)")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("")

        // åˆ›å»ºåˆ†æå™¨
        let analyzer = SCSensitivityAnalyzer()

        // æ£€æŸ¥åˆ†æç­–ç•¥
        let policy = analyzer.analysisPolicy
        if policy == .disabled {
            print(Strings.get("safe.disabled_hint"))
            return
        }

        // åˆ†æå›¾ç‰‡
        let result = try await analyzer.analyzeImage(at: url)

        if format.lowercased() == "json" {
            printJSON(result: result, filename: url.lastPathComponent)
        } else {
            printTable(result: result)
        }
    }

    private func printTable(result: SCSensitivityAnalysis) {
        if result.isSensitive {
            print("âš ï¸  æ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹")
        } else {
            print("âœ… " + Strings.get("safe.is_safe"))
        }
    }

    private func printJSON(result: SCSensitivityAnalysis, filename: String) {
        let dict: [String: Any] = [
            "file": filename,
            "is_sensitive": result.isSensitive
        ]

        if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
           let jsonString = String(data: jsonData, encoding: .utf8) {
            print(jsonString)
        }
    }
}
