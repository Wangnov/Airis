import ArgumentParser
import SensitiveContentAnalysis
import Foundation

struct SafeCommand: AsyncParsableCommand {
    static var configuration: CommandConfiguration {
        CommandConfiguration(
        commandName: "safe",
        abstract: HelpTextFactory.text(
            en: "Detect sensitive content in images",
            cn: "æ£€æµ‹å›¾ç‰‡æ˜¯å¦åŒ…å«æ•æ„Ÿå†…å®¹"
        ),
        discussion: helpDiscussion(
            en: """
                Analyze images for sensitive content (nudity) using Apple's
                SensitiveContentAnalysis framework.

                âš ï¸  AVAILABILITY:
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                This feature is ONLY available in:
                â€¢ App Store version
                â€¢ Development build (build from source with Xcode)

                NOT available in:
                â€¢ Developer ID distribution (Homebrew, GitHub Releases)
                  Due to Apple's provisioning restrictions.
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                REQUIREMENTS:
                1. macOS 14.0 or later
                2. System setting enabled:
                   System Settings > Privacy & Security > Sensitive Content Warning

                QUICK START:
                  airis analyze safe photo.jpg

                EXAMPLES:
                  # Basic sensitive content check
                  airis analyze safe photo.jpg

                  # JSON output for scripting
                  airis analyze safe image.png --format json

                OUTPUT FORMAT (json):
                  {
                    "file": "photo.jpg",
                    "is_sensitive": false
                  }

                PRIVACY NOTES:
                  - All analysis is performed locally on device
                  - Results are never transmitted off-device
                """,
            cn: """
                ä½¿ç”¨ Apple SensitiveContentAnalysis æ¡†æ¶æ£€æµ‹å›¾ç‰‡æ•æ„Ÿå†…å®¹ã€‚

                âš ï¸  å¯ç”¨æ€§è¯´æ˜ï¼š
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                æ­¤åŠŸèƒ½ä»…åœ¨ä»¥ä¸‹ç‰ˆæœ¬å¯ç”¨ï¼š
                â€¢ App Store ç‰ˆæœ¬
                â€¢ Development æ„å»ºï¼ˆä½¿ç”¨ Xcode è‡ªè¡Œç¼–è¯‘ï¼‰

                ä»¥ä¸‹ç‰ˆæœ¬ä¸å¯ç”¨ï¼š
                â€¢ Developer ID åˆ†å‘ç‰ˆæœ¬ï¼ˆHomebrewã€GitHub Releasesï¼‰
                  è¿™æ˜¯ Apple çš„ provisioning é™åˆ¶ã€‚
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                ç³»ç»Ÿè¦æ±‚ï¼š
                1. macOS 14.0+
                2. å¯ç”¨ç³»ç»Ÿè®¾ç½®ï¼š
                   ç³»ç»Ÿè®¾ç½® > éšç§ä¸å®‰å…¨æ€§ > æ•æ„Ÿå†…å®¹è­¦å‘Š

                QUICK START:
                  airis analyze safe photo.jpg

                EXAMPLES:
                  # åŸºç¡€æ£€æµ‹
                  airis analyze safe photo.jpg

                  # JSON è¾“å‡º
                  airis analyze safe image.png --format json

                éšç§è¯´æ˜ï¼š
                  - å…¨éƒ¨æœ¬åœ°æ‰§è¡Œï¼Œä¸ä¸Šä¼ å›¾ç‰‡
                  - ç»“æœä¸ä¼šç¦»å¼€è®¾å¤‡
                """
        )
    )
    }

    @Argument(help: HelpTextFactory.help(en: "Path to the image file", cn: "è¾“å…¥å›¾ç‰‡è·¯å¾„"))
    var imagePath: String

    @Option(name: .long, help: HelpTextFactory.help(en: "Output format: table (default), json", cn: "è¾“å‡ºæ ¼å¼ï¼štableï¼ˆé»˜è®¤ï¼‰æˆ– json"))
    var format: String = "table"

    func run() async throws {
        let testMode = ProcessInfo.processInfo.environment["AIRIS_TEST_MODE"] == "1"
        let forcePolicyDisabled = ProcessInfo.processInfo.environment["AIRIS_SAFE_POLICY_DISABLED"] == "1"
        let forceSensitive = ProcessInfo.processInfo.environment["AIRIS_SAFE_FORCE_SENSITIVE"] == "1"
        let filename = URL(fileURLWithPath: imagePath).lastPathComponent
        let url = try FileUtils.validateImageFile(at: imagePath)

        let outputFormat = OutputFormat.parse(format)
        let showHumanOutput = AirisOutput.shouldPrintHumanOutput(format: outputFormat)

        AirisOutput.printBanner([
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            testMode ? "ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹ (TEST MODE)" : "ğŸ”’ æ•æ„Ÿå†…å®¹æ£€æµ‹",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            "ğŸ“ æ–‡ä»¶: \(url.lastPathComponent)",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        ], enabled: showHumanOutput)

        // é€‰æ‹©åˆ†æå™¨æˆ–æµ‹è¯•æ¡©
        let policy: SCSensitivityAnalysisPolicy
        let isSensitive: Bool

        if testMode {
            policy = forcePolicyDisabled ? .disabled : .simpleInterventions
            isSensitive = forceSensitive
        } else {
            #if DEBUG
            // æµ‹è¯•/è°ƒè¯•æ„å»ºèµ°è½»é‡æ¡©ï¼Œé¿å…ä¾èµ–çœŸå®æ•æ„Ÿå†…å®¹åˆ†æï¼ˆéœ€ç­¾å & ç³»ç»Ÿè®¾ç½®ï¼‰
            policy = .simpleInterventions
            isSensitive = false
            #else
            let analyzer = SCSensitivityAnalyzer()
            policy = analyzer.analysisPolicy
            if policy == .disabled {
                if outputFormat == .json {
                    printDisabledJSON(filename: filename)
                } else if showHumanOutput {
                    print(Strings.get("safe.disabled_hint"))
                }
                return
            }
            let result = try await analyzer.analyzeImage(at: url)
            isSensitive = result.isSensitive
            #endif
        }

        if policy == .disabled {
            // æµ‹è¯•æ¨¡å¼ä¸‹å¼ºåˆ¶è¦†ç›– policy åˆ†æ”¯
            if outputFormat == .json {
                printDisabledJSON(filename: filename)
            } else if showHumanOutput {
                print(Strings.get("safe.disabled_hint"))
            }
            return
        }

        outputResult(isSensitive: isSensitive, filename: filename, outputFormat: outputFormat, showHumanOutput: showHumanOutput)
    }

    private func outputResult(isSensitive: Bool, filename: String, outputFormat: OutputFormat, showHumanOutput: Bool) {
        if outputFormat == .json {
            let dict: [String: Any] = [
                "file": filename,
                "is_sensitive": isSensitive
            ]

            if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
               let jsonString = String(data: jsonData, encoding: .utf8) {
                print(jsonString)
            }
            return
        }

        // table
        guard showHumanOutput else { return }

        if isSensitive {
            print("âš ï¸  æ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹")
        } else {
            print("âœ… " + Strings.get("safe.is_safe"))
        }
    }

    private func printDisabledJSON(filename: String) {
        let dict: [String: Any] = [
            "file": filename,
            "supported": false,
            "error": "policy_disabled"
        ]
        if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
           let jsonString = String(data: jsonData, encoding: .utf8) {
            print(jsonString)
        }
    }
}
