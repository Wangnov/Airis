import ArgumentParser
@preconcurrency import Vision
import Foundation

struct HandCommand: AsyncParsableCommand {
    static var configuration: CommandConfiguration {
        CommandConfiguration(
        commandName: "hand",
        abstract: HelpTextFactory.text(
            en: "Detect hand pose (21 keypoints per hand)",
            cn: "æ£€æµ‹æ‰‹éƒ¨å…³é”®ç‚¹ï¼ˆæ¯åªæ‰‹ 21 ç‚¹ï¼‰"
        ),
        discussion: helpDiscussion(
            en: """
                Detect hand poses in images using Apple's Vision framework.
                Returns 21 keypoints per hand with normalized coordinates.

                QUICK START:
                  airis detect hand photo.jpg

                KEYPOINTS (21 per hand):
                  WRIST:  wrist
                  THUMB:  thumbCMC, thumbMP, thumbIP, thumbTip
                  INDEX:  indexMCP, indexPIP, indexDIP, indexTip
                  MIDDLE: middleMCP, middlePIP, middleDIP, middleTip
                  RING:   ringMCP, ringPIP, ringDIP, ringTip
                  LITTLE: littleMCP, littlePIP, littleDIP, littleTip

                JOINT NAMING:
                  CMC = Carpometacarpal (base)
                  MCP = Metacarpophalangeal (knuckle)
                  MP  = Metacarpophalangeal (thumb knuckle)
                  PIP = Proximal Interphalangeal (middle joint)
                  IP  = Interphalangeal (thumb middle)
                  DIP = Distal Interphalangeal (near tip)
                  Tip = Fingertip

                CHIRALITY:
                  Automatically detects left/right hand

                EXAMPLES:
                  # Basic hand detection
                  airis detect hand gesture.jpg

                  # Detect up to 4 hands
                  airis detect hand group.png --max-hands 4

                  # Show pixel coordinates
                  airis detect hand sign.jpg --pixels

                  # Filter by confidence threshold
                  airis detect hand action.jpg --threshold 0.5

                  # JSON output for scripting
                  airis detect hand pose.jpg --format json

                OUTPUT EXAMPLE:
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                  ğŸ¤š Hand Pose Detection
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                  ğŸ“ File: gesture.jpg
                  ğŸ¯ Threshold: 0.30
                  ğŸ”¢ Max hands: 2
                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

                  Detected 2 hand(s)

                  [1] Right Hand
                      Keypoints (21):
                        wrist:     (0.45, 0.32) - conf: 0.95
                        thumbTip:  (0.52, 0.45) - conf: 0.92
                        indexTip:  (0.48, 0.58) - conf: 0.94
                        ...

                  [2] Left Hand
                      Keypoints (21):
                        wrist:     (0.65, 0.30) - conf: 0.93
                        ...

                OPTIONS:
                  --threshold <val>   Minimum confidence threshold (0.0-1.0, default: 0.3)
                  --max-hands <num>   Maximum number of hands to detect (default: 2)
                  --pixels            Show pixel coordinates instead of normalized
                  --format <fmt>      Output format: table (default), json
                """,
            cn: """
                ä½¿ç”¨ Apple Vision æ¡†æ¶æ£€æµ‹å›¾ç‰‡ä¸­çš„æ‰‹éƒ¨å…³é”®ç‚¹ï¼ˆæ¯åªæ‰‹ 21 ä¸ªç‚¹ï¼‰ã€‚
                é»˜è®¤è¾“å‡ºå½’ä¸€åŒ–åæ ‡ï¼ˆ0.0-1.0ï¼‰ï¼Œå¯ç”¨ --pixels è¾“å‡ºåƒç´ åæ ‡ã€‚

                QUICK START:
                  airis detect hand photo.jpg

                EXAMPLES:
                  # åŸºç¡€æ£€æµ‹
                  airis detect hand gesture.jpg

                  # æœ€å¤šæ£€æµ‹ 4 åªæ‰‹
                  airis detect hand group.png --max-hands 4

                  # è¾“å‡ºåƒç´ åæ ‡
                  airis detect hand sign.jpg --pixels

                  # ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤
                  airis detect hand action.jpg --threshold 0.5

                  # JSON è¾“å‡ºï¼ˆä¾¿äºè„šæœ¬è§£æï¼‰
                  airis detect hand pose.jpg --format json

                OPTIONS:
                  --threshold <val>   ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼Œé»˜è®¤ï¼š0.3ï¼‰
                  --max-hands <num>   æœ€å¤§æ£€æµ‹æ‰‹æ•°ï¼ˆé»˜è®¤ï¼š2ï¼‰
                  --pixels            è¾“å‡ºåƒç´ åæ ‡ï¼ˆé»˜è®¤è¾“å‡ºå½’ä¸€åŒ–ï¼‰
                  --format <fmt>      è¾“å‡ºæ ¼å¼ï¼štableï¼ˆé»˜è®¤ï¼‰æˆ– json
                """
        )
    )
    }

    @Argument(help: HelpTextFactory.help(en: "Path to the image file(s)", cn: "è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼ˆå¯å¤šä¸ªï¼‰"))
    var imagePaths: [String]

    @Option(name: .long, help: HelpTextFactory.help(en: "Minimum confidence threshold (0.0-1.0)", cn: "ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰"))
    var threshold: Float = 0.3

    @Option(name: .long, help: HelpTextFactory.help(en: "Maximum number of hands to detect", cn: "æœ€å¤§æ£€æµ‹æ‰‹æ•°"))
    var maxHands: Int = 2

    @Flag(name: .long, help: HelpTextFactory.help(en: "Show pixel coordinates instead of normalized", cn: "è¾“å‡ºåƒç´ åæ ‡ï¼ˆé»˜è®¤è¾“å‡ºå½’ä¸€åŒ–åæ ‡ï¼‰"))
    var pixels: Bool = false

    @Option(name: .long, help: HelpTextFactory.help(en: "Output format (table, json)", cn: "è¾“å‡ºæ ¼å¼ï¼ˆtable / jsonï¼‰"))
    var format: String = "table"

    func run() async throws {
        let vision = ServiceContainer.shared.visionService
        let outputFormat = OutputFormat.parse(format)
        let showHumanOutput = AirisOutput.shouldPrintHumanOutput(format: outputFormat)

        for imagePath in imagePaths {
            let url = try FileUtils.validateImageFile(at: imagePath)

            // è·å–å›¾åƒå°ºå¯¸ï¼ˆç”¨äºåƒç´ åæ ‡è½¬æ¢ï¼‰
            var imageWidth: Int = 0
            var imageHeight: Int = 0
            if pixels {
                let imageIO = ServiceContainer.shared.imageIOService
                if let info = try? imageIO.getImageInfo(at: url) {
                    imageWidth = info.width
                    imageHeight = info.height
                }
            }

            AirisOutput.printBanner([
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                "ğŸ¤š Hand Pose Detection",
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
                "ğŸ“ File: \(url.lastPathComponent)",
                "ğŸ¯ Threshold: \(String(format: "%.2f", threshold))",
                "ğŸ”¢ Max hands: \(maxHands)",
            ] + ((pixels && imageWidth > 0) ? ["ğŸ“ Image Size: \(imageWidth)Ã—\(imageHeight) px"] : []) + [
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            ], enabled: showHumanOutput)

            // æ‰§è¡Œæ£€æµ‹
            let results = try await vision.detectHumanHandPose(at: url, maximumHandCount: maxHands)

            if results.isEmpty {
                if outputFormat == .json {
                    printJSON(results: [], file: url.lastPathComponent, imageWidth: imageWidth, imageHeight: imageHeight)
                } else if showHumanOutput {
                    print("No hands detected.")
                    print("")
                }
                continue
            }

            // è¾“å‡ºç»“æœ
            if outputFormat == .json {
                printJSON(results: results, file: url.lastPathComponent, imageWidth: imageWidth, imageHeight: imageHeight)
            } else if showHumanOutput {
                printTable(results: results, imageWidth: imageWidth, imageHeight: imageHeight)
            }
        }
    }

    // æ‰€æœ‰æ‰‹éƒ¨å…³é”®ç‚¹åç§°ï¼ˆè®¡ç®—å±æ€§é¿å… Decodable é—®é¢˜ï¼‰
    private var allJointNames: [VNHumanHandPoseObservation.JointName] {
        [
        .wrist,
        .thumbCMC, .thumbMP, .thumbIP, .thumbTip,
        .indexMCP, .indexPIP, .indexDIP, .indexTip,
        .middleMCP, .middlePIP, .middleDIP, .middleTip,
        .ringMCP, .ringPIP, .ringDIP, .ringTip,
        .littleMCP, .littlePIP, .littleDIP, .littleTip
        ]
    }

    private func jointNameString(_ name: VNHumanHandPoseObservation.JointName) -> String {
        switch name {
        case .wrist: return "wrist"
        case .thumbCMC: return "thumbCMC"
        case .thumbMP: return "thumbMP"
        case .thumbIP: return "thumbIP"
        case .thumbTip: return "thumbTip"
        case .indexMCP: return "indexMCP"
        case .indexPIP: return "indexPIP"
        case .indexDIP: return "indexDIP"
        case .indexTip: return "indexTip"
        case .middleMCP: return "middleMCP"
        case .middlePIP: return "middlePIP"
        case .middleDIP: return "middleDIP"
        case .middleTip: return "middleTip"
        case .ringMCP: return "ringMCP"
        case .ringPIP: return "ringPIP"
        case .ringDIP: return "ringDIP"
        case .ringTip: return "ringTip"
        case .littleMCP: return "littleMCP"
        case .littlePIP: return "littlePIP"
        case .littleDIP: return "littleDIP"
        case .littleTip: return "littleTip"
        default: return "unknown"
        }
    }

    private func chiralityString(_ chirality: VNChirality) -> String {
        switch chirality {
        case .left: return "Left Hand"
        case .right: return "Right Hand"
        default: return "Unknown Hand"
        }
    }

    #if DEBUG
    /// æµ‹è¯•è¾…åŠ©ï¼šè¦†ç›–é»˜è®¤åˆ†æ”¯
    static func testJointNameString(_ raw: String) -> String {
        let key = VNRecognizedPointKey(rawValue: raw)
        let name = VNHumanHandPoseObservation.JointName(rawValue: key)
        return HandCommand().jointNameString(name)
    }

    static func testChiralityString(_ value: VNChirality) -> String {
        HandCommand().chiralityString(value)
    }
    #endif

    private func printTable(results: [VNHumanHandPoseObservation], imageWidth: Int, imageHeight: Int) {
        print("Detected \(results.count) hand(s)")
        print("")

        for (index, observation) in results.enumerated() {
            print("[\(index + 1)] \(chiralityString(observation.chirality))")
            print("    Keypoints:")

            for jointName in allJointNames {
                guard let point = try? observation.recognizedPoint(jointName),
                      point.confidence >= threshold else {
                    continue
                }

                let name = jointNameString(jointName)
                let paddedName = name.padding(toLength: 12, withPad: " ", startingAt: 0)

                if pixels && imageWidth > 0 {
                    let px = Int(point.location.x * CGFloat(imageWidth))
                    let py = Int(point.location.y * CGFloat(imageHeight))
                    print("      \(paddedName): (\(px), \(py)) px - conf: \(String(format: "%.2f", point.confidence))")
                } else {
                    let x = String(format: "%.3f", point.location.x)
                    let y = String(format: "%.3f", point.location.y)
                    let conf = String(format: "%.2f", point.confidence)
                    print("      \(paddedName): (\(x), \(y)) - conf: \(conf)")
                }
            }

            print("")
        }
    }

    private func printJSON(results: [VNHumanHandPoseObservation], file: String,
                           imageWidth: Int, imageHeight: Int) {
        let items = results.map { observation -> [String: Any] in
            var keypoints: [[String: Any]] = []

            for jointName in allJointNames {
                guard let point = try? observation.recognizedPoint(jointName),
                      point.confidence >= threshold else {
                    continue
                }

                var keypointDict: [String: Any] = [
                    "name": jointNameString(jointName),
                    "confidence": Double(point.confidence)
                ]

                if pixels && imageWidth > 0 {
                    keypointDict["x"] = Int(point.location.x * CGFloat(imageWidth))
                    keypointDict["y"] = Int(point.location.y * CGFloat(imageHeight))
                    keypointDict["coordinate_type"] = "pixels"
                } else {
                    keypointDict["x"] = Double(point.location.x)
                    keypointDict["y"] = Double(point.location.y)
                    keypointDict["coordinate_type"] = "normalized"
                }

                keypoints.append(keypointDict)
            }

            let chiralityValue = chiralityString(observation.chirality)
                .replacingOccurrences(of: " Hand", with: "")
                .lowercased()

            return [
                "chirality": chiralityValue,
                "keypoint_count": keypoints.count,
                "keypoints": keypoints
            ]
        }

        var dict: [String: Any] = [
            "file": file,
            "count": results.count,
            "threshold": Double(threshold),
            "max_hands": maxHands,
            "hands": items
        ]

        if pixels && imageWidth > 0 {
            dict["image_width"] = imageWidth
            dict["image_height"] = imageHeight
        }

        if let jsonData = try? JSONSerialization.data(withJSONObject: dict, options: [.prettyPrinted, .sortedKeys]),
           let jsonString = String(data: jsonData, encoding: .utf8) {
            print(jsonString)
        }
    }
}
